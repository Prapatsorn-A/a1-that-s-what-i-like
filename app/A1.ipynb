{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Preparation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK dataset\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw text from the Reuters corpus\n",
    "corpus = [reuters.raw(fileid) for fileid in reuters.fileids()]\n",
    "\n",
    "# Split words\n",
    "corpus = [sent.split() for sent in corpus]\n",
    "\n",
    "# Lower case\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word sequences and unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocab = list(set(flatten(corpus)))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numericalization\n",
    "word2index = {w: i for i, w in enumerate(vocab)}\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size\n",
    "voc_size = len(vocab)\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add <UNK> for unknown words\n",
    "vocab.append('<UNK>')\n",
    "vocab[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign index 0 to the <UNK> (unknown)\n",
    "word2index['<UNK>'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Prepare Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in corpus:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, word_sequence, window_size=2):\n",
    "    skip_grams = []  # List to store generated skip-grams\n",
    "    # Iterate through each sentence in the corpus\n",
    "    for sent in corpus:\n",
    "        for i in range(window_size, len(sent) - window_size):\n",
    "            target = word2index[sent[i]] # Target word (current word)\n",
    "            # Collect context words within the window size around the target word\n",
    "            context = [word2index[sent[i - j]] for j in range(1, window_size + 1)] + [word2index[sent[i + j]] for j in range(1, window_size + 1)]\n",
    "            # Create skip-gram pairs (target, context word)\n",
    "            for w in context:\n",
    "                skip_grams.append([target, w])\n",
    "\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) # Randomly pick without replacement\n",
    "    \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]]) # Add target to input batch\n",
    "        random_labels.append([skip_grams[i][1]]) # Add context word to label batch\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 # mini-batch size\n",
    "input_batch, target_batch = random_batch(batch_size, corpus)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "    def forward(self, center_words, target_words, all_vocabs):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
    "        all_embeds = self.embedding_u(all_vocabs) # [batch_size, voc_size, emb_size]\n",
    "        \n",
    "        scores = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        # [batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "\n",
    "        norm_scores = all_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        # [batch_size, voc_size, emb_size] @ [batch_size, emb_size, 1] = [batch_size, voc_size, 1] = [batch_size, voc_size]\n",
    "\n",
    "        nll = -torch.mean(torch.log(torch.exp(scores) / torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
    "        # Loss must be scalar\n",
    "        \n",
    "        return nll # Negative log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 2\n",
    "embedding_size = 2\n",
    "window_size = 2\n",
    "skipgram_model = Skipgram(voc_size, embedding_size)\n",
    "optimizer = optim.Adam(skipgram_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training preparation\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "# Use for the normalized term in the probability calculation\n",
    "all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, len(vocab)) # [batch_size, voc_size]\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for epoch time calculation\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Training\n",
    "num_epochs = 100\n",
    "total_loss = 0.0\n",
    "total_time = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch = random_batch(batch_size, corpus, window_size=window_size)\n",
    "    input_batch = torch.LongTensor(input_batch) # [batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) # [batch_size, 1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = skipgram_model(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    # Update total loss and total time\n",
    "    total_loss += loss.item()\n",
    "    total_time += (end - start)\n",
    "\n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | Loss: {loss:.6f} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "# Loss and Time\n",
    "avg_loss = total_loss / num_epochs\n",
    "\n",
    "total_mins = total_time // 60\n",
    "total_secs = total_time % 60\n",
    "\n",
    "avg_time = total_time / num_epochs\n",
    "avg_mins = int(avg_time // 60)\n",
    "avg_secs = int(avg_time % 60)\n",
    "\n",
    "print(f\"\\nSkipgram Training completed in {total_mins}m {total_secs}s\")\n",
    "print(f\"Skipgram Average Training Time per Epoch: {avg_mins}m {avg_secs}s\")\n",
    "\n",
    "print(f\"\\nSkipgram Total Loss: {total_loss:.6f}\")\n",
    "print(f\"Skipgram Average Loss per Epoch: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Word2Vec (Negative Sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import Libraries and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Prepare Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 0.001 # Scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the occurrences of words in the corpus\n",
    "word_count = Counter(flatten(corpus))\n",
    "num_total_words = sum([c for w, c in word_count.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "for vo in vocab:\n",
    "    unigram_table.extend([vo] * int(((word_count[vo] / num_total_words) ** 0.75) / Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Training preparation: mapping words to indices in a dictionary\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "# Negative sampling function\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.size(0)\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        target_index = targets[i].item()\n",
    "        while len(nsample) < k: # Generate k negative samples\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
    "    return torch.cat(neg_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch  = torch.Tensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neg = 3 # Number of negative samples per target\n",
    "negative_sampling(target_batch, unigram_table, num_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_batch[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size) # context embedding\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "                    \n",
    "    def forward(self, center_words, target_words, negative_words):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
    "        neg_embeds = -self.embedding_u(negative_words) # [batch_size, num_neg, emb_size]\n",
    "        \n",
    "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        negative_score = neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
    "        \n",
    "        loss = self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), 1)\n",
    "                \n",
    "        return -torch.mean(loss)\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        return embeds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 2\n",
    "embedding_size = 2\n",
    "num_neg = 3\n",
    "window_size = 2\n",
    "skipgram_neg_model = SkipgramNegSampling(voc_size, embedding_size)\n",
    "optimizer = optim.Adam(skipgram_neg_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 100\n",
    "total_loss = 0.0\n",
    "total_time = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch = random_batch(batch_size, corpus, window_size=window_size)\n",
    "    input_batch = torch.LongTensor(input_batch) # [batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) # [batch_size, 1]\n",
    "    negs_batch = negative_sampling(target_batch, unigram_table, num_neg) # [batch_size, num_neg]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = skipgram_neg_model(input_batch, target_batch, negs_batch)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update total loss and total time\n",
    "    total_loss += loss.item()\n",
    "    total_time += (end - start)\n",
    "\n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | Loss: {loss:.6f} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "# Loss and Time\n",
    "avg_loss = total_loss / num_epochs\n",
    "\n",
    "total_mins = total_time // 60\n",
    "total_secs = total_time % 60\n",
    "\n",
    "avg_time = total_time / num_epochs\n",
    "avg_mins = int(avg_time // 60)\n",
    "avg_secs = int(avg_time % 60)\n",
    "\n",
    "print(f\"\\nSkipgram Neg Training completed in {total_mins}m {total_secs}s\")\n",
    "print(f\"Skipgram Neg Average Training Time per Epoch: {avg_mins}m {avg_secs}s\")\n",
    "\n",
    "print(f\"\\nSkipgram Neg Total Loss: {total_loss:.6f}\")\n",
    "print(f\"Skipgram Neg Average Loss per Epoch: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import Libraries and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Build Co-occurence Matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipgrams(corpus, window_size=2):\n",
    "    skip_grams = []\n",
    "    for sent in corpus:\n",
    "        for i in range(1, len(sent) - 1):\n",
    "            target = sent[i]\n",
    "            context = sent[max(i - window_size, 0):i] + sent[i + 1:i + window_size + 1]\n",
    "            for w in context:\n",
    "                skip_grams.append((target, w))\n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2  # Default\n",
    "skip_grams = create_skipgrams(corpus, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate co-occurrence counts\n",
    "X_ik_skipgram = Counter(skip_grams)\n",
    "X_ik_skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply a normalized function...don't worry too much\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "        \n",
    "    #check whether the co-occurrences exist between these two words\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  #if does not exist, set it to 1\n",
    "                \n",
    "    x_max = 100 #100 # fixed in paper  #cannot exceed 100 counts\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if co-occurrence does not exceed 100, scale it based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max)**alpha  #scale it\n",
    "    else:\n",
    "        result = 1  #if is greater than max, set it to 1 maximum\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "import numpy as np\n",
    "\n",
    "X_ik = {}  # Keep the co-occurrences (sparse dictionary)\n",
    "weighting_dic = {}  # Dictionary for scaling factors\n",
    "\n",
    "# Instead of looping over all combinations of vocab, only consider co-occurrences in your skip-grams\n",
    "for bigram in combinations_with_replacement(vocab, 2):\n",
    "    if X_ik_skipgram.get(bigram) is not None:  # Matches found in the skip-grams\n",
    "        co_occur = X_ik_skipgram[bigram]  # Get the count from what we already counted\n",
    "        \n",
    "        # Increment co-occurrence count in X_ik (only store when necessary)\n",
    "        X_ik[bigram] = X_ik.get(bigram, 0) + co_occur + 1\n",
    "        X_ik[(bigram[1], bigram[0])] = X_ik.get((bigram[1], bigram[0]), 0) + co_occur + 1\n",
    "\n",
    "        # Apply weighting function and store result in weighting_dic\n",
    "        weighting_value = weighting(bigram[0], bigram[1], X_ik)\n",
    "        weighting_dic[bigram] = weighting_value\n",
    "        weighting_dic[(bigram[1], bigram[0])] = weighting_value  # For symmetry\n",
    "\n",
    "# Print out the resulting dictionaries (will be much smaller than before)\n",
    "print(f\"{X_ik=}\")\n",
    "print(f\"{weighting_dic=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First 10 skip-grams: {list(X_ik_skipgram.items())[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Prepare Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in corpus:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #convert to id since our skip_grams is word, not yet id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_coocs  = []\n",
    "    random_weightings = []\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams_id[i][1]])  # context word, e.g., 3\n",
    "        \n",
    "        #get cooc\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "                    \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 # mini-batch size\n",
    "input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)\n",
    "print(\"Cooc: \", cooc_batch)\n",
    "print(\"Weighting: \", weighting_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size) # out embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs already got log\n",
    "        loss = weighting*torch.pow(inner_product +center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 2\n",
    "embedding_size = 2\n",
    "glove_model = GloVe(voc_size, embedding_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(glove_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 100\n",
    "total_loss = 0.0\n",
    "total_time = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch  = torch.LongTensor(input_batch) #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "    cooc_batch   = torch.FloatTensor(cooc_batch) #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch) #[batch_size, 1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = glove_model(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    # Update total loss and total time\n",
    "    total_loss += loss.item()\n",
    "    total_time += (end - start)\n",
    "\n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | Loss: {loss:.6f} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "# Loss and Time\n",
    "avg_loss = total_loss / num_epochs\n",
    "\n",
    "total_mins = total_time // 60\n",
    "total_secs = total_time % 60\n",
    "\n",
    "avg_time = total_time / num_epochs\n",
    "avg_mins = int(avg_time // 60)\n",
    "avg_secs = int(avg_time % 60)\n",
    "\n",
    "print(f\"\\nGloVe Training completed in {total_mins}m {total_secs:6f}s\")\n",
    "print(f\"GloVe Average Training Time per Epoch: {avg_mins}m {avg_secs:.6f}s\")\n",
    "\n",
    "print(f\"\\nGloVe Total Loss: {total_loss:.6f}\")\n",
    "print(f\"GloVe Average Loss per Epoch: {avg_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Model Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import re\n",
    "\n",
    "glove_file = 'glove.6B.100d.txt'\n",
    "glove_gensim_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the analogies dataset\n",
    "def load_analogies(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Ignore the first line with copyright info\n",
    "    lines = lines[1:]\n",
    "\n",
    "    analogies = {'semantic': [], 'syntactic': []}\n",
    "    current_category = None\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith(':'):\n",
    "            if 'capital-common-countries' in line:\n",
    "                current_category = 'semantic'\n",
    "            elif 'gram7-past-tense' in line:\n",
    "                current_category = 'syntactic'\n",
    "        elif line.strip():\n",
    "            words = line.strip().split()\n",
    "            analogies[current_category].append(words)\n",
    "    \n",
    "    return analogies\n",
    "\n",
    "analogies = load_analogies('word-test.v1.txt')\n",
    "\n",
    "# Solving word analogies using cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return F.cosine_similarity(a, b, dim=1)\n",
    "\n",
    "def solve_analogy(model, word2index, analogy):\n",
    "    # Extract words\n",
    "    a, b, c = analogy\n",
    "    a_idx = word2index.get(a, word2index[\"<UNK>\"])\n",
    "    b_idx = word2index.get(b, word2index[\"<UNK>\"])\n",
    "    c_idx = word2index.get(c, word2index[\"<UNK>\"])\n",
    "    \n",
    "    # Get embeddings\n",
    "    a_embed = model.embedding_v(torch.LongTensor([a_idx])) # [1, emb_size]\n",
    "    b_embed = model.embedding_v(torch.LongTensor([b_idx])) # [1, emb_size]\n",
    "    c_embed = model.embedding_v(torch.LongTensor([c_idx])) # [1, emb_size]\n",
    "    \n",
    "    # Calculate d_embed (d is the missing word in the analogy: a : b :: c : d)\n",
    "    d_embed = b_embed - a_embed + c_embed\n",
    "    \n",
    "    # Find the most similar word to d_embed in the vocabulary\n",
    "    vocab_embeds = model.embedding_v.weight.detach() # [voc_size, emb_size]\n",
    "    similarities = cosine_similarity(d_embed, vocab_embeds)\n",
    "    \n",
    "    # Find the word with the maximum similarity\n",
    "    best_word_idx = torch.argmax(similarities).item()\n",
    "    predicted_word = [word for word, idx in word2index.items() if idx == best_word_idx][0]\n",
    "    \n",
    "    return predicted_word\n",
    "\n",
    "# Calculate accuracy for semantic and syntactic analogies\n",
    "def calculate_accuracy(analogies, model, word2index, category):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for analogy in analogies[category]:\n",
    "        a, b, c, d = analogy  # a : b :: c : d\n",
    "        predicted = solve_analogy(model, word2index, [a, b, c])\n",
    "        if predicted == d:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct / total * 100 if total > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_accuracy_skipgram = calculate_accuracy(analogies, skipgram_model, word2index, 'semantic')\n",
    "print(f\"Semantic Accuracy (SkipGram): {semantic_accuracy_skipgram:.4f}%\")\n",
    "\n",
    "syntactic_accuracy_skipgram = calculate_accuracy(analogies, skipgram_model, word2index, 'syntactic')\n",
    "print(f\"Syntactic Accuracy (SkipGram): {syntactic_accuracy_skipgram:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_accuracy_skipgram_neg = calculate_accuracy(analogies, skipgram_neg_model, word2index, 'semantic')\n",
    "print(f\"Semantic Accuracy (SkipGram with Negative Sampling): {semantic_accuracy_skipgram_neg:.4f}%\")\n",
    "\n",
    "syntactic_accuracy_skipgram_neg = calculate_accuracy(analogies, skipgram_neg_model, word2index, 'syntactic')\n",
    "print(f\"Syntactic Accuracy (SkipGram with Negative Sampling): {syntactic_accuracy_skipgram_neg:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_accuracy_glove = calculate_accuracy(analogies, glove_model, word2index, 'semantic')\n",
    "print(f\"Semantic Accuracy (GloVe): {semantic_accuracy_glove:.4f}%\")\n",
    "\n",
    "syntactic_accuracy_glove = calculate_accuracy(analogies, glove_model, word2index, 'syntactic')\n",
    "print(f\"Syntactic Accuracy (GloVe): {syntactic_accuracy_glove:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analogy dataset\n",
    "def load_analogies(file_path):\n",
    "    analogies = {'semantic': [], 'syntactic': []}\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    current_section = None\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip copyright line\n",
    "        if line.startswith(\"//\") or not line:\n",
    "            continue\n",
    "        \n",
    "        if line.startswith(\":\"):\n",
    "            if \"capital\" in line:\n",
    "                current_section = 'semantic'\n",
    "            elif \"past-tense\" in line:\n",
    "                current_section = 'syntactic'\n",
    "        elif line:\n",
    "            words = re.split(r'\\s+', line)\n",
    "            if len(words) == 4:\n",
    "                analogies[current_section].append(words)\n",
    "    return analogies\n",
    "\n",
    "analogies = load_analogies('word-test.v1.txt')\n",
    "\n",
    "# Calculate the analogy accuracy\n",
    "def calculate_analogy_accuracy(analogies, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for section, items in analogies.items():\n",
    "        for analogy in items:\n",
    "            A, B, C, D_true = analogy\n",
    "            if A not in model or B not in model or C not in model:\n",
    "                continue\n",
    "            \n",
    "            # Perform analogy calculation: vec(B) - vec(A) + vec(C)\n",
    "            vec_analogy = model[B] - model[A] + model[C]\n",
    "            \n",
    "            # Find the closest word to the resulting vector\n",
    "            most_similar = model.similar_by_vector(vec_analogy, topn=10)\n",
    "            \n",
    "            # Check if the most similar word matches D_true\n",
    "            predicted_word = most_similar[0][0]\n",
    "            \n",
    "            if predicted_word.lower() == D_true.lower():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy for both semantic and syntactic analogies\n",
    "semantic_accuracy_glove_gensim = calculate_analogy_accuracy({'semantic': analogies['semantic']}, glove_gensim_model)\n",
    "syntactic_accuracy_glove_gensim = calculate_analogy_accuracy({'syntactic': analogies['syntactic']}, glove_gensim_model)\n",
    "\n",
    "print(f\"Semantic Accuracy (GloVe Gensim): {semantic_accuracy_glove_gensim * 100:.4f}%\")\n",
    "print(f\"Syntactic Accuracy (GloVe Gensim): {syntactic_accuracy_glove_gensim * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load similarity dataset\n",
    "def load_similarity_data(file_path):\n",
    "    word_pairs = []\n",
    "    human_scores = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            word1, word2, score = line.strip().split('\\t')\n",
    "            word_pairs.append((word1, word2))\n",
    "            human_scores.append(float(score))\n",
    "    return word_pairs, np.array(human_scores)\n",
    "\n",
    "# Get word embedding\n",
    "def get_word_embedding(word, word2index, embedding_layer):\n",
    "    idx = word2index.get(word, word2index[\"<UNK>\"])\n",
    "    return embedding_layer(torch.LongTensor([idx]))  # [1, emb_size]\n",
    "\n",
    "# Calculate cosine similarity\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    dot_product = torch.dot(embedding1.squeeze(), embedding2.squeeze())\n",
    "    norm1 = torch.norm(embedding1)\n",
    "    norm2 = torch.norm(embedding2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Main procedure\n",
    "def run_similarity_evaluation(similarity_file, skipgram_model, word2index):\n",
    "    word_pairs, human_scores = load_similarity_data(similarity_file)\n",
    "    \n",
    "    predicted_scores = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        embedding1 = get_word_embedding(word1, word2index, skipgram_model.embedding_v)\n",
    "        embedding2 = get_word_embedding(word2, word2index, skipgram_model.embedding_v)\n",
    "        sim = cosine_similarity(embedding1, embedding2)\n",
    "        predicted_scores.append(sim.item())  # Convert tensor to float\n",
    "    \n",
    "    # Calculate Spearman correlation and MSE\n",
    "    correlation, _ = spearmanr(predicted_scores, human_scores)\n",
    "    mse_skipgram = mean_squared_error(human_scores, predicted_scores)\n",
    "    print(f\"Skipgram Spearman Correlation: {correlation:.4f}\")\n",
    "    print(f\"Skipgram MSE: {mse_skipgram:.4f}\")\n",
    "    print(f\"Y_true MSE: {mean_squared_error(human_scores, human_scores):.4f}\")  # This is trivially 0, as Y_true is itself.\n",
    "\n",
    "# Usage\n",
    "similarity_file = 'wordsim_similarity_goldstandard.txt'\n",
    "run_similarity_evaluation(similarity_file, skipgram_model, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main procedure\n",
    "def run_similarity_evaluation(similarity_file, skipgram_neg_model, word2index):\n",
    "    word_pairs, human_scores = load_similarity_data(similarity_file)\n",
    "    \n",
    "    predicted_scores = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        embedding1 = get_word_embedding(word1, word2index, skipgram_neg_model.embedding_v)\n",
    "        embedding2 = get_word_embedding(word2, word2index, skipgram_neg_model.embedding_v)\n",
    "        sim = cosine_similarity(embedding1, embedding2)\n",
    "        predicted_scores.append(sim.item())  # Convert tensor to float\n",
    "    \n",
    "    # Calculate Spearman correlation and MSE\n",
    "    correlation, _ = spearmanr(predicted_scores, human_scores)\n",
    "    mse_skipgram_neg = mean_squared_error(human_scores, predicted_scores)\n",
    "\n",
    "    print(f\"Skipgram with Negative Sampling Spearman Correlation: {correlation:.4f}\")    \n",
    "    print(f\"Skipgram with Negative Sampling MSE: {mse_skipgram_neg:.4f}\")\n",
    "\n",
    "# Usage\n",
    "similarity_file = 'wordsim_similarity_goldstandard.txt'\n",
    "run_similarity_evaluation(similarity_file, skipgram_neg_model, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main procedure\n",
    "def run_similarity_evaluation(similarity_file, glove_model, word2index):\n",
    "    word_pairs, human_scores = load_similarity_data(similarity_file)\n",
    "    \n",
    "    predicted_scores = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        embedding1 = get_word_embedding(word1, word2index, glove_model.embedding_v)\n",
    "        embedding2 = get_word_embedding(word2, word2index, glove_model.embedding_v)\n",
    "        sim = cosine_similarity(embedding1, embedding2)\n",
    "        predicted_scores.append(sim.item())  # Convert tensor to float\n",
    "    \n",
    "    # Calculate Spearman correlation and MSE\n",
    "    correlation, _ = spearmanr(predicted_scores, human_scores)\n",
    "    mse_glove = mean_squared_error(human_scores, predicted_scores)\n",
    "\n",
    "    print(f\"GloVe Spearman Correlation: {correlation:.4f}\")    \n",
    "    print(f\"GloVe MSE: {mse_glove:.4f}\")\n",
    "\n",
    "# Usage\n",
    "similarity_file = 'wordsim_similarity_goldstandard.txt'\n",
    "run_similarity_evaluation(similarity_file, glove_model, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load similarity dataset\n",
    "def load_similarity_data(file_path):\n",
    "    word_pairs = []\n",
    "    human_scores = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            word1, word2, score = line.strip().split('\\t')\n",
    "            word_pairs.append((word1, word2))\n",
    "            human_scores.append(float(score))\n",
    "    return word_pairs, np.array(human_scores)\n",
    "\n",
    "# Get word embedding\n",
    "def get_word_embedding(word, embedding_layer):\n",
    "    # Check if word exists in Gensim model\n",
    "    if word in embedding_layer:\n",
    "        return torch.tensor(embedding_layer[word], dtype=torch.float32)  # convert Gensim word vector to a torch tensor\n",
    "    else:\n",
    "        # Return a zero vector if the word is not found\n",
    "        return torch.zeros(embedding_layer.vector_size, dtype=torch.float32)  # handle unknown words\n",
    "\n",
    "# Calculate cosine similarity\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    dot_product = torch.dot(embedding1.squeeze(), embedding2.squeeze())\n",
    "    norm1 = torch.norm(embedding1)\n",
    "    norm2 = torch.norm(embedding2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Main procedure\n",
    "def run_similarity_evaluation(similarity_file, glove_gensim_model):\n",
    "    word_pairs, human_scores = load_similarity_data(similarity_file)\n",
    "    \n",
    "    predicted_scores = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        embedding1 = get_word_embedding(word1, glove_gensim_model)\n",
    "        embedding2 = get_word_embedding(word2, glove_gensim_model)\n",
    "        # Calculate similarity\n",
    "        sim = cosine_similarity(embedding1, embedding2)\n",
    "        predicted_scores.append(sim.item())  # Convert tensor to float\n",
    "    \n",
    "    # Remove NaN or invalid values\n",
    "    valid_indices = ~np.isnan(predicted_scores) & ~np.isnan(human_scores)\n",
    "    human_scores_valid = human_scores[valid_indices]\n",
    "    predicted_scores_valid = np.array(predicted_scores)[valid_indices]\n",
    "    \n",
    "    # Calculate Spearman Correlation and MSE\n",
    "    correlation, _ = spearmanr(predicted_scores_valid, human_scores_valid)\n",
    "    mse_glove_gensim = mean_squared_error(human_scores_valid, predicted_scores_valid)\n",
    "    print(f\"GloVe Gensim Spearman Correlation: {correlation:.4f}\")\n",
    "    print(f\"GloVe Gensim MSE: {mse_glove_gensim:.4f}\")\n",
    "\n",
    "# Usage\n",
    "similarity_file = 'wordsim_similarity_goldstandard.txt'\n",
    "run_similarity_evaluation(similarity_file, glove_gensim_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(skipgram_model.state_dict(), \"skipgram_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('word2index.pkl', 'wb') as f:\n",
    "    pickle.dump(word2index, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
